---
title: "K-Means Clustering - Data Science Capstone"
author: "Siva Gopavarapu"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Week 2

Summary - Paper 1

Clustering is a technique which helps us to group data by calculating
the similarity between them and is used widely in the field of machine
learning. There are many techniques in clustering and we’re going to
concentrate on K-Means here.

K-Means is an unsupervised machine learning algorithm. It categorizes
data based on the distance between the points.  The process starts with
taking a K value (centroid point) to divide the data into K categories,
data points are assigned to each category by calculating and comparing
the distance between each K and the data point. Distance is measured
using the Euclidean method.

A data point is assigned to cluster where the Euclidean diatance is the
least.

The final goal here is that the distance between the datapoint and the
centroid of that cluster is minimized. This can be achieved by the
following steps:

1.       Initiate the K values as per the goal.

2.       Calculate the distance of each point to these K values and
assign them to the nearest K

3.       Once all the data points are assigned to a K, calculate the new
centroid of the clusters, and calculate the distance between each point
and the new centroid again. Now, assign the points to the nearest K.

4.       Perform steps 2 and 3 until the centroid value doesn’t change
anymore.

One problem which is faced here is how many K values should be
initialized to begin with. This can be solved using Elbow method.  The
whole idea of the Elbow method is that the sum of distance between the
point and centroid of a group decreases as we increase the number of
centroids(K). To find the optimal number of centroids, we need to plot a
graph where we can visually see the elbow point i.e optimal number of K
values where there is no significant reduction in the sum of distances
with increase in K value. By doing this, we can identify the optimal
number of clusters for our data set.

[@cui2020introduction]

Summary - Paper 2

In this paper, data of the transactions of a Logistics company from
Middle East is analyzed using K-Means clustering to answer the question
“How can customer segmentation from the data can be used to monetize?” 
Before preparing the final data set to input into K-means algorithm,
data preparation has been performed such as cleaning, merging, and
preprocessing the data which includes standardizing the values and
creating the new relevant variables.

Tableau is used to visualize the data in the first step before running
K-means algorithm to get better understanding of the data. Graphs are
built to understand any visual patterns between customers, product
categories, share of e-commerce companies in each product category, and
the destination countries.

As the K value need to be initialized in the first place, experiment has
been ran on 2 to 5 clusters to find the perfect fit of customer groups.
Data from each K value clusters has been analyzed based on variables
such as number of items, avg total value, product group, country of
destination, etc. Calinski-Harabasz criterion is used to assess the
quality of clusters, higher the value here tells us that the clusters
are well separated. ANOVA statistic has been used to validate the best
fit solution from Calinski-Harabasz criterion. After observing the p
value \<0.001 of Total USD variable, it can be confirmed that the
clusters are statistically different. Also, the two clusters in 5
cluster solution, and one cluster in 4 cluster solution has very less
data points which led us to choose k=3 as the best fit solution.

Companies getting to know more about their customer groups based on the
average order value, products mostly bought, and the location of
delivery can benefit them to market their product effectively to the
right set of customers leading to more business and profits.

[@qabbaah2019using]

## Week 3

Summary – Paper 1 (Cluster Application with K-Means Algorithm on the
Population of Trade and Accommodation Facilities in Indonesia)

Goal of the paper:

Goal of the paper is to categorize the regions in Indonesia into three
clusters namely K1(high), K2(medium), K3(low) using k-means algorithm
based on the trade (6 attributes – mini markets, restaurants, food and
beverage shops, shops, hotels, and lodging) and accommodation facilities
and help government identify the to pay more attention and try to
improve the overall performance in terms of trade, accommodation, and
other facilities. By understanding the position of each region,
government needs to maximize the potential of each region and educate
the importance clean water, and other basic facilities to the people.
Data is collected from the website [www.bps.go.id](http://www.bps.go.id)
which provides government statistical data.

This research might be very important both for government and the people
of Indonesia to improve the overall trade performance and people lives.

Methodology:

Data of 6 attributes for 34 provinces is collected from the
above-mentioned link.

K-Means –

From the data of 6 attributes and 34 provinces, average values of the 6
attributes (mini markets, restaurants, food and beverage shops, shops,
hotels, lodging) in the first step to make it easier for group the data
into clusters.

Once the average value of each attribute is processed, Rapid miner
software is used to process the data into three clusters (high, medium,
and low)

Initializing centroid values – The value for the cluster K1 is obtained
the maximum value of a record, K2 is obtained from average value, and K3
from lowest value of a record.

After forming the initial clusters, centroids value are re-calculated
and update the cluster until the last cluster is same as the previous
value. Finally, each province is assigned to a cluster where 5 belong to
high, 13 are classified as medium, and 16 are classified as low.

From the results, we can see that there are many provinces in Indonesia
which needs a lot of improvement in trade and accommodation. For better
results, compare the results from other clustering methods.

[@munawar2021cluster]

Summary – Paper 2(MapReduce Design of K-Means Clustering Algorithm)

Goal of the paper:

Goal of the paper is to implement the K-Means clustering algorithm in a
distributed environment using the MapReduce framework in Apache Hadoop
ecosystem. With tremendous increase in volume of data in the recent
days, distributed computing helps us to perform the analysis and build
complex model much quicker and helps companies act quicker on the data
to improve their business.

As the process of clustering and K-means is already discussed in the
paper summaries in last week, I’ll be concentrating on MapReduce and
using K-Means in distributed computing environment in this paper
summary.

Methodology:

Distributed computing – It’s a technique which helps to solve complex
computational processing by distributing the load on multiple nodes
within a cluster. To maintain the nodes within a cluster, Apache Hadoop
came into picture which follows MapReduce programming paradigm. This
framework also takes care of node failure and data backup which makes it
flexible to run complex computational tasks. All this efficiency is
achieved by sharing the data and computational tasks between multiple
nodes within a cluster.

MapReduce programming process starts with dividing large sets of
datasets into chunks and then sent to multiple nodes in the cluster
(size of each chunk depends on the number of nodes and the capacity).
Map function outputs the data in the form of a key value pair which is
then sent as an input to the reduce function to combine the results
having the same key value. Both map and reduce function can be custom
written by the programmer keeping the fundamental principles in place.
Hadoop frame also has a job tracker and a master node to check the
status of each node performance and the output.

K-Means clustering using MapReduce – The first step is to prepare the
data as key value pairs where key is the centroid value and value is the
data point value. Two files are created where one contains the centroid
values and the other contains data point values. Now, the mapper
function is coded to calculate the distance between each key(centroid)
and the data point value and then assign it to the key with the lowest
distance. Once all the points are assigned to a cluster, centroid value
is recalculated in the reduce function. A new iteration of map reduce
will be performed again until the centroid value is not updated.

Though the MapReduce framework help us in building the K-means much
quicker with huge datasets, it definitely helps to choose the correct
number of initial centroids to reduce the number of iterations before
arriving at the final clusters.

Reference - Anchalia, P. P., Koundinya, A. K., & Srinath, N. K. (2013,
June). MapReduce design of K-means clustering algorithm. In 2013
International Conference on Information Science and Applications (ICISA)
(pp. 1-5). IEEE.

## **Week 4**

Summary – Paper 1 (Extended K-Means Algorithm)

Goal – As we know the K-means algorithm try to minimize the within
cluster sum of squares distance and doesn’t take the number of elements
in each cluster into consideration, but in certain areas such as
logistics management, or if the user decides that minimum no of elements
in a cluster is necessary. This can be achieved by an extended K-means
algorithm discussed in this paper.

Methodology – Extended K-means is designed based on the greedy
algorithm. Below steps gives us the overall idea of it

1 – Apply K-means algorithm on the prepared data to obtain the initial
set of clusters

2 – Sort the clusters from K-means in ascending order based on the
number of elements in each cluster

3 – Based on the predefined number of elements in each cluster, we start
either recruiting or dropping elements from the first cluster from the
sorted clusters.

4 – If there are n elements in the cluster and we require a element,
calculate the difference a-n, we need to recruit points from other
cluster if it’s positive else drop the points from this cluster

5 – When recruiting a-n elements, points with the shortest distance are
chosen excluding the points already in the cluster. When dropping a-n
elements, points whose distance is closest to center of the other
cluster are dropped. This is a greedy approach as we’re trying to
satisfy the minimum no of points in each cluster.

6 – After step 5, the K–means algorithm is again applied on the
remaining data points to form k-1 clusters. Repeat steps 2 to 5 till you
reach one cluster from K-means algorithm.

[@6642738]

Summary Paper – 2 (Genetic K-means Algorithm for Credit Card Fraud
Detection)

Goal – With the rapid increase of internet usage and credit card
payments, frauds associated with it also increase. Proper security
measures need to be taken to avoid merchants needing to bear the cost of
fraudulent transactions and pay the customers back. Between 2006 to
2007, the US has seen an increase of 1.7 billion. To prevent these
fraudulent transactions happening, algorithms used by companies need to
have very less execution time with good accuracy, this paper uses
Genetic K-Means algorithm.

Methodology –

Genetic Algorithm – It follows survival of the fittest rule of natural
selection which has three steps i.e.

Selection – It calculates the fitness of individuals from each
generation.

Crossover – It helps us to generate new individuals by combining the
individuals from each generation.

Mutation – It performs random modifications on the newly generated
individuals from the crossover step.

This approach makes use of fitness score and inherit good properties of
parents when producing new generations.

Proposed system to detect fraudulent transactions.

Once the dataset is loaded, transaction rules such as amount spent,
authentication used, location, and spending history, etc are applied and
the transaction summary of all the values is computed. Now, K-Means
clustering is applied on the critical values of each transaction which
will form three clusters namely low, medium, and high.

In the next step, genetic algorithm selects the fittest points from the
clusters to produce new generation by performing single point crossover
and one-bit flip mutation until the best results are found and classify
the transaction as critical, monitorable, and ordinary.

This methodology has been tested on a database with 1500 transactions
according to certain rules such as Average daily spending, CC Usage
Frequency, CC Usage Location, Proxy Port check, IP Address Check, Wrong
Password Attempt Check, Authentication Type Check, CC Balance, CC
Overdraft to calculate the percentage of critical records found.

From the results, it’s observed that there’s a difference of 11 sec
lesser when we use Generic K-means algorithm instead of Simple Genetic
algorithm.

[@chougule2015genetic]
